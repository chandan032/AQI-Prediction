# -*- coding: utf-8 -*-
"""Copy of xgboost_regressor_modified.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VgxSHpPdlePrivbehvk5pXahEa7d1nyD
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn

!pip install xgboost==1.3.3

import xgboost as xgb

print(xgb.__version__)

combined_data = pd.read_csv('cleaned_data.csv')

combined_data.head()

combined_data.columns

X = combined_data.iloc[:, :-1]
y = combined_data.iloc[:, -1]

data_dmatrix = xgb.DMatrix(data=X,label=y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)

"""## XGBOOST REGRESSOR"""

# from xgboost import XGBRegressor
# xgb = XGBRegressor()
# xgb.fit(X_train, y_train)

xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,
                max_depth = 5, alpha = 10, n_estimators = 10,gpu_id=0,tree_method = "gpu_hist",)

xg_reg.fit(X_train,y_train)

preds = xg_reg.predict(X_test)

rmse = np.sqrt(sklearn.metrics.mean_squared_error(y_test, preds))
print("RMSE: %f" % (rmse))

params = {"objective":"reg:linear",'colsample_bytree': 0.3,'learning_rate': 0.1,
                'max_depth': 5, 'alpha': 10, 'tree_method':'gpu_hist','gpu_id':0}

cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,
                    num_boost_round=50,early_stopping_rounds=10,metrics="rmse", as_pandas=True, seed=43)

cv_results.head()

print((cv_results["test-rmse-mean"]).tail(1))

xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)

import matplotlib.pyplot as plt

xgb.plot_tree(xg_reg,num_trees=0)
# plt.rcParams['figure.figsize'] = [50, 10]
plt.show()
plt.savefig('treeplot')

xgb.plot_importance(xg_reg)
plt.rcParams['figure.figsize'] = [5, 5]
plt.show()

# store the winning model in a new variable
_
# saving the feature names to the model

# Create the feature importances plot
fig, ax = plt.subplots(1, 3, figsize=(15,5))
# plot importances with feature weight
xgb.plot_importance(
    booster=xg_reg, 
    importance_type='weight',
    title='Feature Weight',
    show_values=False,
    height=0.5,
    ax=ax[0],
)
# plot importances with split mean gain
xgb.plot_importance(
    booster=xg_reg,
    importance_type='gain',
    title='Split Mean Gain',
    show_values=False,
    height=0.5,
    ax=ax[1]
)
# plot importances with sample coverage
xgb.plot_importance(
    xg_reg,
    importance_type='cover',
    title='Sample Coverage',
    show_values=False,
    height=0.5,
    ax=ax[2]
)
plt.tight_layout()
plt.show()
plt.savefig('metrics')

# Create the feature importances plot
fig, ax = plt.subplots(figsize=(20,20))
# plot a decision tree from the booster
xgb.plot_tree(booster=xg_reg, num_trees=0, ax=ax, rankdir='LR')
plt.tight_layout()
plt.show()
plt.savefig('treeplot_horizontal')

f'Coefficient of determination R^2 on train set {xgb.score(X_train, y_train)}'
# must be close to 1, 1 is perfect fit

f'Coefficient of determination R^2 on test set {xgb.score(X_test, y_test)}'

"""### OVERFIT MODEL"""

from sklearn.model_selection import cross_val_score
score = cross_val_score(xgb, X, y, cv = 3)

score.mean()

pred = xgb.predict(X_test)

sns.distplot(y_test - pred)

"""#### HYPERPARAMETER TUNING"""

n_estimators = [int(x) for x in np.linspace(start=100, stop=1200, num=12)]
learning_rate = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
max_depth = [int(x) for x in np.linspace(5, 30, num=6)]
subsample = [0.7, 0.6, 0.8]
min_child_weight = list(range(3, 8))
objective = ['reg:squarederror']
params = {
    'n_estimators': n_estimators,
    'learning_rate': learning_rate,
    'max_depth': max_depth,
    'subsample': subsample,
    'min_child_weight': min_child_weight,
    'objective': objective
}

from sklearn.model_selection import RandomizedSearchCV
search = RandomizedSearchCV(xgb, params, scoring='neg_mean_squared_error', 
                            cv=5, n_iter=100, random_state=43, n_jobs=-1, verbose=True)
search.fit(X,y)

search.best_params_

search.best_score_

pred = search.predict(X_test)
sns.distplot(y_test-pred)

from sklearn import metrics
print(f"Mean Abs Error: {metrics.mean_absolute_error(y_test, pred)}")
print(f"Mean Sq Error: {metrics.mean_squared_error(y_test, pred)}")
print(f"Root Mean Error: {np.sqrt(metrics.mean_squared_error(y_test, pred))}")

search.best_estimator_

xgb.get_booster().feature_names

final_features=[25,34,14,23,45,56,1,0]
x=np.asarray(final_features).reshape(1,-1)
xgb.predict(x)

import pickle
pickle.dump(search, open('xgb_model2.pkl', 'wb'))

import pickle
model = pickle.load(open('xgb_model.pkl', "rb"))
pred=model.predict([[25,34,14,23,45,56,1,0]])
print(pred)